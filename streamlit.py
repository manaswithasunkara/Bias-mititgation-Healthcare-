import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.metrics import accuracy_score, classification_report

st.set_page_config(layout="wide")
st.title("ðŸ“Š NHANES Insurance Prediction and Health Data Exploration")

@st.cache_data
def load_dataset(file):
    return pd.read_sas(file, format="xport")

# Sidebar file uploads
st.sidebar.header("Upload NHANES .xpt Files")
files = {}
for label in ["BPXO_L", "DEMO_L", "FERTIN_L", "HIQ_L", "HSCRP_L", "KIQ_U_L", "BMX_L", "VID_L", "UCPREG_L"]:
    uploaded_file = st.sidebar.file_uploader(f"Upload {label}.xpt", type="xpt")
    if uploaded_file:
        files[label] = load_dataset(uploaded_file)

# Data processing and merging
if len(files) >= 3:
    st.subheader("âœ… Loaded Datasets")
    for name, df in files.items():
        st.write(f"**{name}**")
        st.dataframe(df.head())

    # Merge datasets on SEQN
    data = files["DEMO_L"]
    for name, df in files.items():
        if name != "DEMO_L":
            data = pd.merge(data, df, on="SEQN", how="inner")

    data = data.drop_duplicates(subset="SEQN")
    data.columns = data.columns.str.strip().str.lower()

    # Drop columns with >50% missing
    missing_percent = data.isnull().mean()
    cols_to_drop = missing_percent[missing_percent > 0.5].index.tolist()
    data = data.drop(columns=cols_to_drop)

    # Impute numeric columns
    numeric_cols = data.select_dtypes(include=["int64", "float64"]).columns
    num_imputer = SimpleImputer(strategy="median")
    data[numeric_cols] = num_imputer.fit_transform(data[numeric_cols])

    # Filter females only (if gender present)
    if "riagendr" in data.columns:
        data = data[data["riagendr"] != 1.0]

    # Target: Insurance-related columns
    target_cols = [col for col in data.columns if col.startswith("hiq")]

    st.markdown("### ðŸŽ¯ Insurance-related Target Columns")
    st.write(target_cols)

    # Define features and labels
    x = data.drop(columns=target_cols, errors="ignore")
    y = data[target_cols]

    if not y.empty:
        # Preprocessing
        scaler = StandardScaler()
        x_scaled = scaler.fit_transform(x.select_dtypes(include=["int64", "float64"]))

        poly = PolynomialFeatures(degree=2, include_bias=False)
        x_poly = poly.fit_transform(x_scaled)

        x_train, x_test, y_train, y_test = train_test_split(x_poly, y, test_size=0.3, random_state=0)

        # Train logistic regression
        base_model = LogisticRegression(max_iter=1000)
        model = MultiOutputClassifier(base_model)
        model.fit(x_train, y_train)
        y_pred = model.predict(x_test)

        st.markdown("### ðŸ“ˆ Accuracy per Target")
        for i, col in enumerate(y.columns):
            acc = accuracy_score(y_test[col], y_pred[:, i])
            st.write(f"**{col}: {acc:.4f}**")

        st.markdown("### ðŸ§¾ Classification Reports")
        for i, col in enumerate(y.columns):
            report = classification_report(y_test[col], y_pred[:, i], output_dict=True, zero_division=1)
            st.write(f"**{col}**")
            st.json(report)

        # ðŸ”¥ Feature Importance via Random Forest
        st.markdown("### ðŸŒ² Feature Importances (RandomForestClassifier)")
        rf = RandomForestClassifier(n_estimators=100, random_state=0)
        rf.fit(x_scaled, y[target_cols[0]])  # use first target for simplicity
        importances = rf.feature_importances_

        # Map back to original feature names
        feature_names = poly.get_feature_names_out(input_features=x.select_dtypes(include=["int64", "float64"]).columns)
        importance_df = pd.DataFrame({
            "Feature": feature_names,
            "Importance": importances
        }).sort_values(by="Importance", ascending=False).head(20)

        fig, ax = plt.subplots(figsize=(10, 6))
        sns.barplot(data=importance_df, x="Importance", y="Feature", ax=ax)
        st.pyplot(fig)

        # ðŸ“Š Correlation Heatmap
        st.markdown("### ðŸ”¥ Correlation Heatmap of Numeric Features")
        corr = data[numeric_cols].corr()
        fig, ax = plt.subplots(figsize=(15, 10))
        sns.heatmap(corr, cmap="coolwarm", center=0, annot=False, ax=ax)
        st.pyplot(fig)

    else:
        st.warning("No insurance-related columns found in merged data.")

else:
    st.info("Upload at least 3 .xpt NHANES files to begin.")
